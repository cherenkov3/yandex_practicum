{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для интернет-магазина"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv('/datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(comments.info())\n",
    "display(comments.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделим признаки и целевой признак - toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = comments['toxic']\n",
    "features = comments['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим датасет на обучающую и тестовую выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем данные к нужному типу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = features_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test = features_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оставим в датасете только регулярные выражения. Воспользуемся методом re.sub(), применим его ко всему датасету"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    text_lem = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    return text_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corpus_train)):\n",
    "    corpus_train[i] = clear_text(corpus_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corpus_test)):\n",
    "    corpus_test[i] = clear_text(corpus_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью стемминга приведем слова к форме основы. Воспользуемся инструментом EnglishStemmer из библиотеки nltk. Применим ко всему датасету."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = EnglishStemmer(ignore_stopwords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 46s, sys: 219 ms, total: 1min 47s\n",
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(len(corpus_train)):\n",
    "    word_list = nltk.word_tokenize(corpus_train[i])\n",
    "    corpus_train[i] = ' '.join([stemmer.stem(w) for w in word_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.7 s, sys: 55.2 ms, total: 26.8 s\n",
      "Wall time: 26.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(len(corpus_test)):\n",
    "    word_list = nltk.word_tokenize(corpus_test[i])\n",
    "    corpus_test[i] = ' '.join([stemmer.stem(w) for w in word_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим базу стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Векторизуем данные с учетом стоп-слов. Применим метод fit к обычающей выборке, а затем метод transform к обучающему и тестовому набору данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "tf_idf = count_tf_idf.fit(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = tf_idf.transform(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = tf_idf.transform(corpus_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Матрицы готовы, обучим на них модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=12345, solver = 'liblinear', class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced', random_state=12345,\n",
       "                   solver='liblinear')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pd.Series(model.predict(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(predicted, target_test).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем метод кросс-валидации для подбора гиперпараметров модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score при max_depth = 5 : [0.32775423 0.32857649 0.31931464]\n",
      "Score mean = 0.32521512110120193\n",
      "\n",
      "Score при max_depth = 10 : [0.33856109 0.36025567 0.35878234]\n",
      "Score mean = 0.3525330320879141\n",
      "\n",
      "Score при max_depth = 15 : [0.38828338 0.37879027 0.36143862]\n",
      "Score mean = 0.3761707556864591\n",
      "\n",
      "Score при max_depth = 20 : [0.39804546 0.38129228 0.38553694]\n",
      "Score mean = 0.3882915615603882\n",
      "\n",
      "Score при max_depth = 25 : [0.43001565 0.42040352 0.41945946]\n",
      "Score mean = 0.42329287515685027\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#исследуем работу модели с различными значениями гиперпараметра max_depth\n",
    "for max_depth in range(5, 26, 5):\n",
    "    rf_model = RandomForestClassifier(class_weight = 'balanced', max_depth = max_depth, n_estimators = 100)\n",
    "    rf_cv = cross_val_score(rf_model, train_x, target_train, cv=3,scoring='f1')\n",
    "    print(\"Score при max_depth =\", max_depth, \":\", rf_cv)\n",
    "    print(\"Score mean =\", sum(rf_cv)/len(rf_cv))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score при n_estimators = 50 : [0.31530298 0.29994639 0.31231385]\n",
      "Score mean = 0.3091877393417795\n",
      "\n",
      "Score при n_estimators = 100 : [0.32679114 0.33567766 0.33988493]\n",
      "Score mean = 0.334117909838656\n",
      "\n",
      "Score при n_estimators = 150 : [0.33963614 0.32672029 0.32525217]\n",
      "Score mean = 0.33053619659885086\n",
      "\n",
      "Score при n_estimators = 200 : [0.34545866 0.3148997  0.34318276]\n",
      "Score mean = 0.33451370773639394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#исследуем работу модели с различными значениями гиперпараметра n_estimators\n",
    "for estim in range(50, 201, 50):\n",
    "    rf_model = RandomForestClassifier(class_weight = 'balanced', max_depth = 5, n_estimators = estim)\n",
    "    rf_cv = cross_val_score(rf_model, train_x, target_train, cv=3,scoring='f1')\n",
    "    print(\"Score при n_estimators =\", estim, \":\", rf_cv)\n",
    "    print(\"Score mean =\", sum(rf_cv)/len(rf_cv))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения модели возьмем значение гиперпараметра max_depth = 25 и n_estimators = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = RandomForestClassifier(class_weight = 'balanced',max_depth = 25,n_estimators = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced', max_depth=25, n_estimators=200)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.fit(train_x, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_1 = pd.Series(model_1.predict(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(predicted_1, target_test).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значение метрики F1 достигло необходимого значения только у модели LogisticRegression. Модель Random Forest Classifier не справилась с поставленной задачей.\n",
    "\n",
    "Из-за большого количества наблюдений и их сложной структуры, мощности обычного компьютера периодически не хватает для полноценного анализа, из-за этого время обучения и работы возрастает, и не все модели можно детально рассмотреть."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 4,
    "start_time": "2022-07-24T17:04:34.975Z"
   },
   {
    "duration": 827,
    "start_time": "2022-07-24T17:04:36.151Z"
   },
   {
    "duration": 37,
    "start_time": "2022-07-24T17:04:37.775Z"
   },
   {
    "duration": 3,
    "start_time": "2022-07-24T17:06:02.036Z"
   },
   {
    "duration": 17,
    "start_time": "2022-07-24T17:06:38.380Z"
   },
   {
    "duration": 954,
    "start_time": "2022-07-24T17:07:06.072Z"
   },
   {
    "duration": 980,
    "start_time": "2022-07-24T17:07:19.726Z"
   },
   {
    "duration": 3,
    "start_time": "2022-07-24T17:07:50.482Z"
   },
   {
    "duration": 2798,
    "start_time": "2022-07-24T17:08:04.508Z"
   },
   {
    "duration": 2787,
    "start_time": "2022-07-24T17:08:18.027Z"
   },
   {
    "duration": 3,
    "start_time": "2022-07-24T17:08:49.818Z"
   },
   {
    "duration": 70910,
    "start_time": "2022-07-24T17:09:05.695Z"
   },
   {
    "duration": 70906,
    "start_time": "2022-07-24T17:10:16.607Z"
   },
   {
    "duration": 329,
    "start_time": "2022-07-24T17:11:27.515Z"
   },
   {
    "duration": 4944,
    "start_time": "2022-07-24T17:11:27.847Z"
   },
   {
    "duration": 4608,
    "start_time": "2022-07-24T17:11:32.794Z"
   },
   {
    "duration": 4534,
    "start_time": "2022-07-24T17:11:37.405Z"
   },
   {
    "duration": 3,
    "start_time": "2022-07-24T17:17:50.353Z"
   },
   {
    "duration": 9597,
    "start_time": "2022-07-24T17:18:04.382Z"
   },
   {
    "duration": 12,
    "start_time": "2022-07-24T17:18:20.815Z"
   },
   {
    "duration": 36,
    "start_time": "2022-07-24T17:18:37.356Z"
   },
   {
    "duration": 210182,
    "start_time": "2022-07-24T17:19:46.248Z"
   },
   {
    "duration": 69323,
    "start_time": "2022-07-24T17:23:16.432Z"
   },
   {
    "duration": 3,
    "start_time": "2022-07-24T17:25:36.495Z"
   },
   {
    "duration": 4492,
    "start_time": "2022-07-24T17:25:50.591Z"
   },
   {
    "duration": 784,
    "start_time": "2022-07-24T17:26:06.847Z"
   },
   {
    "duration": 26,
    "start_time": "2022-07-24T17:26:28.816Z"
   },
   {
    "duration": 4,
    "start_time": "2022-07-24T17:26:44.495Z"
   },
   {
    "duration": 17684,
    "start_time": "2022-07-24T17:26:46.256Z"
   },
   {
    "duration": 2845,
    "start_time": "2022-07-24T17:27:05.445Z"
   },
   {
    "duration": 33,
    "start_time": "2022-07-24T17:27:09.756Z"
   },
   {
    "duration": 60,
    "start_time": "2022-07-27T17:13:54.341Z"
   },
   {
    "duration": 5,
    "start_time": "2022-07-27T17:15:24.561Z"
   },
   {
    "duration": 827,
    "start_time": "2022-07-27T17:15:27.069Z"
   },
   {
    "duration": 30,
    "start_time": "2022-07-27T17:15:28.753Z"
   },
   {
    "duration": 2,
    "start_time": "2022-07-27T17:15:33.574Z"
   },
   {
    "duration": 13,
    "start_time": "2022-07-27T17:15:38.081Z"
   },
   {
    "duration": 17,
    "start_time": "2022-07-27T17:16:25.186Z"
   },
   {
    "duration": 1653,
    "start_time": "2022-07-27T17:16:32.933Z"
   },
   {
    "duration": 300,
    "start_time": "2022-07-27T17:28:18.064Z"
   },
   {
    "duration": 15,
    "start_time": "2022-07-27T17:29:19.820Z"
   },
   {
    "duration": 15,
    "start_time": "2022-07-27T17:29:32.832Z"
   },
   {
    "duration": 15,
    "start_time": "2022-07-27T17:29:35.256Z"
   },
   {
    "duration": 15,
    "start_time": "2022-07-27T17:29:50.044Z"
   },
   {
    "duration": 98,
    "start_time": "2022-07-27T17:30:50.418Z"
   },
   {
    "duration": 17,
    "start_time": "2022-07-27T17:30:53.282Z"
   },
   {
    "duration": 2,
    "start_time": "2022-07-27T17:30:58.742Z"
   },
   {
    "duration": 2,
    "start_time": "2022-07-27T17:31:07.494Z"
   },
   {
    "duration": 3627,
    "start_time": "2022-07-27T17:31:08.306Z"
   },
   {
    "duration": 1286,
    "start_time": "2022-07-27T17:31:13.001Z"
   },
   {
    "duration": 3,
    "start_time": "2022-07-27T17:34:05.125Z"
   },
   {
    "duration": 3,
    "start_time": "2022-07-27T17:34:07.061Z"
   },
   {
    "duration": 3628,
    "start_time": "2022-07-27T17:34:07.829Z"
   },
   {
    "duration": 1102,
    "start_time": "2022-07-27T17:34:11.459Z"
   },
   {
    "duration": 2,
    "start_time": "2022-07-27T17:34:21.669Z"
   },
   {
    "duration": 3,
    "start_time": "2022-07-27T17:34:24.669Z"
   },
   {
    "duration": 3509,
    "start_time": "2022-07-27T17:34:25.569Z"
   },
   {
    "duration": 659,
    "start_time": "2022-07-27T17:34:29.080Z"
   },
   {
    "duration": 3,
    "start_time": "2022-07-27T17:34:41.369Z"
   },
   {
    "duration": 3,
    "start_time": "2022-07-27T17:34:42.560Z"
   },
   {
    "duration": 2,
    "start_time": "2022-07-27T17:34:45.885Z"
   },
   {
    "duration": 2546,
    "start_time": "2022-07-27T17:34:46.704Z"
   },
   {
    "duration": 638,
    "start_time": "2022-07-27T17:34:50.205Z"
   },
   {
    "duration": 3,
    "start_time": "2022-07-27T17:34:53.598Z"
   },
   {
    "duration": 107264,
    "start_time": "2022-07-27T17:35:01.576Z"
   },
   {
    "duration": 26851,
    "start_time": "2022-07-27T17:36:48.842Z"
   },
   {
    "duration": 390,
    "start_time": "2022-07-27T17:37:27.786Z"
   },
   {
    "duration": 5456,
    "start_time": "2022-07-27T17:39:21.596Z"
   },
   {
    "duration": 5255,
    "start_time": "2022-07-27T17:39:29.300Z"
   },
   {
    "duration": 1265,
    "start_time": "2022-07-27T17:39:35.540Z"
   },
   {
    "duration": 4,
    "start_time": "2022-07-27T17:40:02.635Z"
   },
   {
    "duration": 14474,
    "start_time": "2022-07-27T17:40:05.340Z"
   },
   {
    "duration": 6,
    "start_time": "2022-07-27T17:40:19.816Z"
   },
   {
    "duration": 14,
    "start_time": "2022-07-27T17:40:22.283Z"
   },
   {
    "duration": 396918,
    "start_time": "2022-07-27T17:40:54.315Z"
   },
   {
    "duration": 131811,
    "start_time": "2022-07-27T17:54:15.346Z"
   },
   {
    "duration": 409402,
    "start_time": "2022-07-27T17:56:27.159Z"
   },
   {
    "duration": 68,
    "start_time": "2022-07-27T18:03:16.570Z"
   },
   {
    "duration": 0,
    "start_time": "2022-07-27T18:03:16.640Z"
   },
   {
    "duration": 14,
    "start_time": "2022-07-27T18:03:44.017Z"
   },
   {
    "duration": 3,
    "start_time": "2022-07-27T18:03:47.875Z"
   },
   {
    "duration": 29695,
    "start_time": "2022-07-27T18:03:48.861Z"
   },
   {
    "duration": 1193,
    "start_time": "2022-07-27T18:04:31.077Z"
   },
   {
    "duration": 27,
    "start_time": "2022-07-27T18:04:34.393Z"
   },
   {
    "duration": 87,
    "start_time": "2022-07-28T10:52:08.505Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
